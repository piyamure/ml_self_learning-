{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the loads to word array \n",
    "words = open('names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first 10 chars \n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n"
     ]
    }
   ],
   "source": [
    "# length of the word array \n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A : Bi-gram level predictions based on the bi-gram frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length of a word in the words array = 15\n",
      "min length of a word in the words array = 2\n"
     ]
    }
   ],
   "source": [
    "# max and min word length \n",
    "print(f'max length of a word in the words array = {max(len(w) for w in words)}')\n",
    "print(f'min length of a word in the words array = {min(len(w) for w in words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create character bi-gram dictionary with bi-gram frequancy \n",
    "b = {}\n",
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1,ch2 in zip(chs,chs[1:]):\n",
    "        bigram = (ch1,ch2)\n",
    "        b[bigram] = b.get(bigram,0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('n', '<E>'), 6763),\n",
       " (('a', '<E>'), 6640),\n",
       " (('a', 'n'), 5438),\n",
       " (('<S>', 'a'), 4410),\n",
       " (('e', '<E>'), 3983),\n",
       " (('a', 'r'), 3264),\n",
       " (('e', 'l'), 3248),\n",
       " (('r', 'i'), 3033),\n",
       " (('n', 'a'), 2977),\n",
       " (('<S>', 'k'), 2963),\n",
       " (('l', 'e'), 2921),\n",
       " (('e', 'n'), 2675),\n",
       " (('l', 'a'), 2623),\n",
       " (('m', 'a'), 2590),\n",
       " (('<S>', 'm'), 2538),\n",
       " (('a', 'l'), 2528),\n",
       " (('i', '<E>'), 2489),\n",
       " (('l', 'i'), 2480),\n",
       " (('i', 'a'), 2445),\n",
       " (('<S>', 'j'), 2422)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 20 bi-grams which has highest occurance (freq.)\n",
    "sorted(b.items(),key = lambda kv : -kv[1])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make encoder (chars->int) and decoder (int->chars)\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] =0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will store the bi-gram freq. (first char along the row and second along the col.)\n",
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']   # start and end with char \".\"\n",
    "    for ch1,ch2 in zip(chs,chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1,ix2] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "          134,  535,  929],\n",
       "        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n",
       "         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n",
       "          182, 2050,  435],\n",
       "        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,\n",
       "          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,\n",
       "            0,   83,    0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1377, 0.0408, 0.0481, 0.0528, 0.0478, 0.0130, 0.0209, 0.0273,\n",
       "        0.0184, 0.0756, 0.0925, 0.0491, 0.0792, 0.0358, 0.0123, 0.0161, 0.0029,\n",
       "        0.0512, 0.0642, 0.0408, 0.0024, 0.0117, 0.0096, 0.0042, 0.0167, 0.0290])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is showing the probability of all the characters that can come after letter '.', i.e. N[0]\n",
    "p = N[0].float()\n",
    "p = p/p.sum()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want to find the next char after letter 'a'\n",
    "g = torch.Generator().manual_seed(2147483647)   # this is to make same output \n",
    "ix = torch.multinomial(p,num_samples=1,replacement=True,generator=g).item()\n",
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P                   : 27,27\n",
    "# P.sum(1,keepdims=T) : 27,1\n",
    "# According to broadcasting rules this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1192e-05, 1.3759e-01, 4.0767e-02, 4.8129e-02, 5.2745e-02, 4.7785e-02,\n",
      "        1.3038e-02, 2.0898e-02, 2.7293e-02, 1.8465e-02, 7.5577e-02, 9.2452e-02,\n",
      "        4.9064e-02, 7.9195e-02, 3.5777e-02, 1.2321e-02, 1.6095e-02, 2.9008e-03,\n",
      "        5.1154e-02, 6.4130e-02, 4.0830e-02, 2.4641e-03, 1.1759e-02, 9.6070e-03,\n",
      "        4.2109e-03, 1.6719e-02, 2.9008e-02])\n"
     ]
    }
   ],
   "source": [
    "P = (N+1).float()   # here we add 1 to remve the zero probablities \n",
    "P /= P.sum(1,keepdims=True)\n",
    "print(P[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cony.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "# let's try to generate few words completely based on the bi-gram probabilities (or count freq.)\n",
    "g = torch.Generator().manual_seed(2147483647)  # generator \n",
    "\n",
    "for i in range(5):                             # generate 5 words \n",
    "    out = []\n",
    "    ix = 0                                     # start with char '.'\n",
    "    while True:\n",
    "        p = P[ix]                              # pluck one raw of P \n",
    "        ix = torch.multinomial(p,num_samples=1,replacement=True,generator=g).item() # get the next char index\n",
    "        out.append(itos[ix])                   # append the next char to \"out\" list\n",
    "        \n",
    "        if ix ==0:                             # stop generating more chars if the current char is '.'\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Bi-gram level predictions using gradient descent optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL: maximize the likelyhood of data w.r.t. model parameters (statistical modeling)\n",
    "# this is equivalent to the maximizing log likelyhood (note that log is monotonic)\n",
    "# maximizing log-likelyhood is same as minimizing negetive log likelyhood\n",
    "# so the final minimization is minimize the average negative log -likelyhood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avergae negative log likelyhood :2.4543561935424805\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1,ch2 in zip(chs,chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1,ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n+=1\n",
    "print(f'avergae negative log likelyhood :{-log_likelihood/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# let's create training set of bigrams (x,y)\n",
    "\n",
    "xs,ys = [],[]\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1,ch2 in zip(chs,chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1,ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the input (we have to one-hot encode)\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the target (output) of the network \n",
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's one-hot encode the input \n",
    "import torch.nn.functional as F \n",
    "xenc = F.one_hot(xs,num_classes=27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4cfd939640>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.3077e-01, -6.4540e-01, -1.2990e+00,  1.8234e-02, -4.0779e-01,\n",
       "          2.1252e+00, -1.6899e+00,  1.1917e+00,  1.1961e+00, -3.6455e-01,\n",
       "         -6.9090e-02,  1.3015e+00, -8.1208e-01, -4.1094e-01, -1.7527e-01,\n",
       "         -1.2484e+00,  6.9964e-01,  9.2687e-01,  1.0867e+00,  1.1805e-03,\n",
       "         -1.0427e+00,  7.2859e-02, -8.6493e-01, -1.7336e-01, -3.2089e-01,\n",
       "         -1.2865e-01,  8.8367e-01],\n",
       "        [ 1.1091e+00,  9.0565e-01, -5.7649e-01,  3.4041e-01,  3.9209e-01,\n",
       "         -9.7180e-02, -1.4673e+00, -3.7074e-01, -1.1074e+00, -8.1330e-01,\n",
       "         -1.6823e-01,  2.3594e-01,  3.4943e-01,  5.3269e-01,  1.0863e+00,\n",
       "         -4.2405e-01, -7.8057e-01,  4.9633e-01, -1.8923e-01,  1.5615e+00,\n",
       "         -6.5979e-01,  7.8055e-01,  5.6760e-01, -6.7544e-02,  9.6104e-01,\n",
       "         -1.2404e+00,  2.2822e-02],\n",
       "        [-9.1065e-01, -9.3891e-01, -9.9417e-01,  6.2410e-01,  7.7323e-01,\n",
       "         -1.1691e-01,  1.0978e+00, -1.9726e+00, -1.0948e+00, -9.6604e-01,\n",
       "         -1.9925e+00, -4.1975e-01,  6.8306e-02,  1.1985e+00,  1.6452e+00,\n",
       "          8.2582e-01,  7.4132e-02, -8.7700e-01,  1.0133e+00, -1.7381e+00,\n",
       "          1.0763e+00, -6.1258e-02, -1.0198e+00, -3.2543e-01,  1.3294e+00,\n",
       "          1.0266e-01,  1.3316e-01],\n",
       "        [-9.1065e-01, -9.3891e-01, -9.9417e-01,  6.2410e-01,  7.7323e-01,\n",
       "         -1.1691e-01,  1.0978e+00, -1.9726e+00, -1.0948e+00, -9.6604e-01,\n",
       "         -1.9925e+00, -4.1975e-01,  6.8306e-02,  1.1985e+00,  1.6452e+00,\n",
       "          8.2582e-01,  7.4132e-02, -8.7700e-01,  1.0133e+00, -1.7381e+00,\n",
       "          1.0763e+00, -6.1258e-02, -1.0198e+00, -3.2543e-01,  1.3294e+00,\n",
       "          1.0266e-01,  1.3316e-01],\n",
       "        [-1.4360e+00,  2.0962e-01,  9.0163e-02, -1.1997e+00, -3.6406e-01,\n",
       "         -5.5851e-02,  5.9127e-01, -1.1857e+00,  4.3888e-01,  1.6511e+00,\n",
       "         -1.9021e+00,  2.0001e-01, -1.3111e+00,  5.5553e-02,  1.4802e+00,\n",
       "         -9.1944e-01,  1.0010e+00,  7.9998e-03, -6.3231e-01,  1.0814e+00,\n",
       "         -2.0955e+00, -1.0601e-01, -5.1379e-01,  6.0989e-01, -3.5076e-01,\n",
       "         -1.1785e+00,  2.7744e+00]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight matrix (input layer), the input size = 27 (chars) and lets make output size = 27 \n",
    "# output_size = 27 is a fair argument (at least for now), as we insert one char (out of 27) and need to output another char (out of 27)\n",
    "\n",
    "W = torch.randn((27,27))  # weight matrix (we initialize it with random numbers)\n",
    "xenc @ W                  # this is the output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0097, 0.0129, 0.0067, 0.0250, 0.0163, 0.2053, 0.0045, 0.0807, 0.0811,\n",
       "         0.0170, 0.0229, 0.0901, 0.0109, 0.0163, 0.0206, 0.0070, 0.0494, 0.0619,\n",
       "         0.0727, 0.0245, 0.0086, 0.0264, 0.0103, 0.0206, 0.0178, 0.0216, 0.0593],\n",
       "        [0.0810, 0.0661, 0.0150, 0.0376, 0.0395, 0.0242, 0.0062, 0.0184, 0.0088,\n",
       "         0.0118, 0.0226, 0.0338, 0.0379, 0.0455, 0.0792, 0.0175, 0.0122, 0.0439,\n",
       "         0.0221, 0.1273, 0.0138, 0.0583, 0.0471, 0.0250, 0.0699, 0.0077, 0.0273],\n",
       "        [0.0106, 0.0103, 0.0097, 0.0491, 0.0570, 0.0234, 0.0789, 0.0037, 0.0088,\n",
       "         0.0100, 0.0036, 0.0173, 0.0282, 0.0873, 0.1364, 0.0601, 0.0283, 0.0109,\n",
       "         0.0725, 0.0046, 0.0772, 0.0248, 0.0095, 0.0190, 0.0995, 0.0292, 0.0301],\n",
       "        [0.0106, 0.0103, 0.0097, 0.0491, 0.0570, 0.0234, 0.0789, 0.0037, 0.0088,\n",
       "         0.0100, 0.0036, 0.0173, 0.0282, 0.0873, 0.1364, 0.0601, 0.0283, 0.0109,\n",
       "         0.0725, 0.0046, 0.0772, 0.0248, 0.0095, 0.0190, 0.0995, 0.0292, 0.0301],\n",
       "        [0.0049, 0.0254, 0.0225, 0.0062, 0.0143, 0.0195, 0.0372, 0.0063, 0.0319,\n",
       "         0.1073, 0.0031, 0.0251, 0.0055, 0.0218, 0.0904, 0.0082, 0.0560, 0.0207,\n",
       "         0.0109, 0.0607, 0.0025, 0.0185, 0.0123, 0.0379, 0.0145, 0.0063, 0.3299]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's convert logits (network -outputs) into probabilities\n",
    "\n",
    "logits = xenc @ W # logits, log-counts \n",
    "counts = logits.exp() # this is equivalent to counts N\n",
    "probs = counts/counts.sum(1,keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "# forward pass\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(5),ys].log().mean() # this is the loss (negative log-likelyhood OR cross entrophy) \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass, we want to find the gradient of the loss w.r.t weights \n",
    "W.grad = None \n",
    "loss.backward()\n",
    "W.data += -0.1*W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# let's optimize the weights of the net using gradient-descent and get the bigram level predictions\n",
    "\n",
    "# step 01: create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.509854793548584\n",
      "2.5089924335479736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5081682205200195\n",
      "2.507380247116089\n",
      "2.5066258907318115\n",
      "2.5059030055999756\n",
      "2.5052108764648438\n",
      "2.5045459270477295\n",
      "2.503908157348633\n",
      "2.503295421600342\n",
      "2.502706289291382\n",
      "2.5021398067474365\n",
      "2.5015945434570312\n",
      "2.5010693073272705\n",
      "2.500562906265259\n",
      "2.500075578689575\n",
      "2.4996049404144287\n",
      "2.499150514602661\n",
      "2.4987120628356934\n",
      "2.49828839302063\n",
      "2.4978787899017334\n",
      "2.497483015060425\n",
      "2.4970998764038086\n",
      "2.4967286586761475\n",
      "2.496370315551758\n",
      "2.496022939682007\n",
      "2.4956860542297363\n",
      "2.4953596591949463\n",
      "2.4950432777404785\n",
      "2.4947361946105957\n",
      "2.494438648223877\n",
      "2.494149684906006\n",
      "2.4938690662384033\n",
      "2.4935967922210693\n",
      "2.4933321475982666\n",
      "2.493074893951416\n",
      "2.4928252696990967\n",
      "2.492582321166992\n",
      "2.4923462867736816\n",
      "2.492116928100586\n",
      "2.4918932914733887\n",
      "2.491675853729248\n",
      "2.491464376449585\n",
      "2.491258144378662\n",
      "2.491058111190796\n",
      "2.4908626079559326\n",
      "2.4906723499298096\n",
      "2.4904870986938477\n",
      "2.4903063774108887\n",
      "2.4901304244995117\n"
     ]
    }
   ],
   "source": [
    "# step 02: gradient descent (after this weights are optimized to match the targets)\n",
    "for k in range(50):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morvann.\n",
      "akela.\n",
      "az.\n",
      "arileri.\n",
      "chaiadayra.\n"
     ]
    }
   ],
   "source": [
    "# step 03: sample from the 'neural-net' model \n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0 \n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]),num_classes=27).float()\n",
    "        logits = xenc @ W \n",
    "        counts = logits.exp()\n",
    "        p = counts/counts.sum(1,keepdims=True)\n",
    "        \n",
    "        ix = torch.multinomial(p,num_samples=1,replacement=True,generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        \n",
    "        if ix ==0:\n",
    "            break \n",
    "    print(''.join(out))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
