{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "\n",
    "import torch.nn as nn \n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device agnostic code \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds \n",
    "\n",
    "def set_seeds(seed:int = 42):\n",
    "    # set the seed for general torch operations \n",
    "    torch.manual_seed(seed)\n",
    "    # set the seed for CUDA torch operations \n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data (We have already downoad the data)\n",
    "\n",
    "import os \n",
    "from pathlib import Path \n",
    "\n",
    "data_path = Path('DATA')\n",
    "image_path = data_path/\"pizza_steak_sushi\"\n",
    "\n",
    "# set up train and test directories \n",
    "train_dir = image_path/'train'\n",
    "test_dir = image_path/'test'\n",
    "\n",
    "# setup pretrined weights \n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "\n",
    "# get the transforms from weights \n",
    "automatic_transforms = weights.transforms()\n",
    "\n",
    "# create data loaders \n",
    "from going_modular import data_setup\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir, test_dir=test_dir,transform=automatic_transforms, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model with trnsfer learning  \n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "# freeze all base layers \n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False \n",
    "    \n",
    "# and update the classifier head to suit our problem \n",
    "model.classifier = torch.nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=len(class_names),bias=True)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
       "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
       "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
       "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n",
       "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
       "│    └─Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n",
       "============================================================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (G): 12.31\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.09\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3487.41\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the model summary \n",
    "from torchinfo import summary\n",
    "summary(model=model,input_size=(32,3,224,224), col_names=['input_size','output_size','num_params','trainable'],\n",
    "        col_width=20,row_settings=['var_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimizer \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "from going_modular.engine import train_step, test_step\n",
    "from typing import Dict, List \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def train(model:torch.nn.Module,\n",
    "          train_dataloader:torch.utils.data.DataLoader,\n",
    "          test_dataloader:torch.utils.data.DataLoader,\n",
    "          optimizer:torch.optim.Optimizer,\n",
    "          loss_fn:torch.nn.Module,\n",
    "          epochs:int,\n",
    "          device:torch.device) -> Dict[str,List]:\n",
    "    # create empty results dictionary \n",
    "    results = {\"train_loss\":[],\"train_acc\":[],\n",
    "               \"test_loss\":[],\"test_acc\":[]}\n",
    "    \n",
    "    # loop through training and testing steps for a number of epochs \n",
    "    for epoch in range(epochs):\n",
    "        train_loss,train_acc = train_step(model=model, dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,optimizer=optimizer,device=device)\n",
    "        test_loss,test_acc = test_step(model=model,dataloader=test_dataloader,\n",
    "                                       loss_fn=loss_fn,device=device)\n",
    "        \n",
    "        # print what is happening \n",
    "        print(f\"Epoch:{epoch+1} | train_loss:{train_loss:.4f} | train_acc:{train_acc:.4f}\\\n",
    "            | test_loss:{test_loss:.4f} | test_acc:{test_acc:.4f}\")\n",
    "        \n",
    "        # save the results to dictionary \n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        # experimental tracking \n",
    "        writer.add_scalars(main_tag=\"Loss\",tag_scalar_dict={\"train_loss\":train_loss, \"test_loss\":test_loss},global_step=epoch)\n",
    "        writer.add_scalars(main_tag=\"Accuracy\",tag_scalar_dict={\"train_acc\":train_acc,\"test_acc\":test_acc},global_step=epoch)\n",
    "        writer.add_graph(model=model,input_to_model=torch.randn(32,3,224,224).to(device))\n",
    "        \n",
    "    writer.close()\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 | train_loss:1.1141 | train_acc:0.3867            | test_loss:0.9437 | test_acc:0.5786\n",
      "Epoch:2 | train_loss:0.9072 | train_acc:0.6484            | test_loss:0.8260 | test_acc:0.7936\n",
      "Epoch:3 | train_loss:0.8040 | train_acc:0.7500            | test_loss:0.7084 | test_acc:0.8542\n",
      "Epoch:4 | train_loss:0.6661 | train_acc:0.8867            | test_loss:0.6983 | test_acc:0.8144\n",
      "Epoch:5 | train_loss:0.6986 | train_acc:0.7266            | test_loss:0.7046 | test_acc:0.7538\n"
     ]
    }
   ],
   "source": [
    "# train the model \n",
    "set_seeds()\n",
    "results = train(model=model,train_dataloader=train_dataloader,test_dataloader=test_dataloader,\n",
    "                optimizer=optimizer,loss_fn=loss_fn,epochs=5,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom summary writer instance \n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def create_writer(experiment_name:str,model_name:str, extra:str=None)-> torch.utils.tensorboard.writer.SummaryWriter():\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    if extra:\n",
    "        log_dir = os.path.join(\"runs\",timestamp,experiment_name,model_name,extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\",timestamp,experiment_name,model_name)\n",
    "        \n",
    "    print(f\"[INFO] created SummaryWriter, saving to:{log_dir}\")\n",
    "    \n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] created SummaryWriter, saving to:runs/2024-06-12/data_10_percent/effnetb0/5_epochs\n"
     ]
    }
   ],
   "source": [
    "# create example writer \n",
    "example_writer = create_writer(experiment_name=\"data_10_percent\",model_name=\"effnetb0\",extra=\"5_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's include write function inside the train function \n",
    "\n",
    "def train(model:torch.nn.Module,\n",
    "          train_dataloader:torch.utils.data.DataLoader,\n",
    "          test_dataloader:torch.utils.data.DataLoader,\n",
    "          optimizer:torch.optim.Optimizer,\n",
    "          loss_fn:torch.nn.Module,\n",
    "          epochs:int,\n",
    "          device:torch.device,\n",
    "          writer:torch.utils.tensorboard.writer.SummaryWriter) -> Dict[str,List]:\n",
    "    # create empty results dictionary \n",
    "    results = {\"train_loss\":[],\"train_acc\":[],\n",
    "               \"test_loss\":[],\"test_acc\":[]}\n",
    "    \n",
    "    # loop through training and testing steps for a number of epochs \n",
    "    for epoch in range(epochs):\n",
    "        train_loss,train_acc = train_step(model=model, dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,optimizer=optimizer,device=device)\n",
    "        test_loss,test_acc = test_step(model=model,dataloader=test_dataloader,\n",
    "                                       loss_fn=loss_fn,device=device)\n",
    "        \n",
    "        # print what is happening \n",
    "        print(f\"Epoch:{epoch+1} | train_loss:{train_loss:.4f} | train_acc:{train_acc:.4f}\\\n",
    "            | test_loss:{test_loss:.4f} | test_acc:{test_acc:.4f}\")\n",
    "        \n",
    "        # save the results to dictionary \n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        \n",
    "        # experimental tracking \n",
    "        if writer:\n",
    "            writer.add_scalars(main_tag=\"Loss\",tag_scalar_dict={\"train_loss\":train_loss, \"test_loss\":test_loss},global_step=epoch)\n",
    "            writer.add_scalars(main_tag=\"Accuracy\",tag_scalar_dict={\"train_acc\":train_acc,\"test_acc\":test_acc},global_step=epoch)\n",
    "            writer.add_graph(model=model,input_to_model=torch.randn(32,3,224,224).to(device))\n",
    "            \n",
    "            writer.close()\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run with 20% data, with 10 epochs in model effnet_0 \n",
    "epochs = 10 \n",
    "train(model=model,train_dataloader=train_dataloader,test_dataloader=test_dataloader,\n",
    "      optimizer=optimizer,loss_fn=loss_fn,epochs=epochs,device=device,\n",
    "      writer= create_writer(experiment_name='20_percent_data',model_name=\"effnetb0\",extra=f\"{epochs}_epochs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
